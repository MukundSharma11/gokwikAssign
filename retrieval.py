from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain.retrievers.document_compressors.chain_extract import LLMChainExtractor
from langchain_community.document_transformers.long_context_reorder import LongContextReorder
from typing import List, Dict, Any
import logging

class RetrievalPipeline:
    def __init__(
        self,
        vector_db,
        llm,
        compression_llm=None,
        k_documents=5,
        diversity_threshold=0.7
    ):
        self.vector_db = vector_db
        self.llm = llm
        self.compression_llm = compression_llm or llm
        self.k_documents = k_documents
        self.diversity_threshold = diversity_threshold
        
        self._setup_retrieval_pipeline()
    
    def _setup_retrieval_pipeline(self):
        """Set up the pipeline with MMR applied to each generated query."""
        
        if self.vector_db is None:
            logging.error("Vector database not initialized!")
            return
        
        # STEP 1: Create MMR-enabled base retriever
        # This will apply MMR to EACH query generated by MultiQueryRetriever
        self.mmr_retriever = self.vector_db.as_retriever(
            search_type="mmr",
            search_kwargs={
                'k': self.k_documents,
                'fetch_k': self.k_documents * 3,  # Fetch more candidates for MMR selection
                'lambda_mult': self.diversity_threshold  # Balance relevance vs diversity
            }
        )
        
        # STEP 2: Create MultiQueryRetriever with MMR base retriever
        # This is the RIGHT approach: query rewriting + MMR per generated query
        self.multi_query_retriever = MultiQueryRetriever.from_llm(
            retriever=self.mmr_retriever,  # MMR applied to each generated query
            llm=self.llm,
            parser_key="lines"
        )
        
        # STEP 3: Document reordering
        self.reorder_transformer = LongContextReorder()
        
        # STEP 4: Contextual compression
        compressor = LLMChainExtractor.from_llm(self.compression_llm)
        self.compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=self.multi_query_retriever
        )
    
    def retrieve_with_all_steps(self, user_query: str) -> List[Dict[str, Any]]:
        """
        Complete retrieval pipeline: Query rewriting + MMR per query + Re-ranking + Compression
        """
        try:
            if self.vector_db is None:
                logging.error("Vector database not initialized!")
                return []
                
            logging.info(f"Starting retrieval pipeline for query: {user_query}")
            
            # STEP 1: Multi-query retrieval with MMR applied to each generated query
            # - LLM generates multiple query variations
            # - Each variation gets MMR-based diverse retrieval
            # - Results are combined and deduplicated
            multi_query_mmr_docs = self.multi_query_retriever.get_relevant_documents(user_query)
            logging.info(f"Multi-query + MMR retrieved {len(multi_query_mmr_docs)} documents")
            
            # STEP 2: Re-ranking with LongContextReorder
            # Optimizes document order for better context utilization
            reordered_docs = self.reorder_transformer.transform_documents(multi_query_mmr_docs)
            logging.info(f"Re-ordered {len(reordered_docs)} documents")
            
            # STEP 3: Contextual compression
            # Extracts most relevant parts of documents for the query
            compressed_docs = self.compression_retriever.get_relevant_documents(user_query)
            logging.info(f"Applied contextual compression, final count: {len(compressed_docs)}")
            
            # Format final results
            results = []
            for i, doc in enumerate(compressed_docs):
                # Get source filename from metadata
                source = doc.metadata.get('source', 'Unknown')
                if source != 'Unknown':
                    source = source.split('/')[-1]  # Get filename only
                
                results.append({
                    'rank': i + 1,
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'source': source,
                    'processing_steps': 'MultiQuery+MMR → Re-ranking → Compression'
                })
            
            logging.info(f"Pipeline completed successfully. Returning {len(results)} final documents")
            return results
            
        except Exception as e:
            logging.error(f"Error in retrieval pipeline: {str(e)}")
            # Fallback to simple retrieval if advanced pipeline fails
            try:
                docs_and_scores = self.vector_db.similarity_search_with_score(
                    user_query, 
                    k=self.k_documents
                )
                results = []
                for i, (doc, score) in enumerate(docs_and_scores):
                    source = doc.metadata.get('source', 'Unknown')
                    if source != 'Unknown':
                        source = source.split('/')[-1]
                    
                    results.append({
                        'rank': i + 1,
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'source': source,
                        'processing_steps': f'Fallback retrieval (score: {score:.4f})'
                    })
                return results
            except Exception as fallback_error:
                logging.error(f"Fallback retrieval also failed: {str(fallback_error)}")
                return []